@InProceedings{Tschuggnall2014,
  author    = {Tschuggnall, Michael and Specht, Günther},
  title     = {{Enhancing Authorship Attribution By Utilizing Syntax Tree Profiles}},
  booktitle = {Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL'2014)},
  year      = {2014},
  volume    = {2},
  pages     = {195--199},
  publisher = {Association for Computational Linguistics},
  isbn      = {978-1-937284-78-7},
}

@InProceedings{Tschuggnall2013a,
  author    = {Tschuggnall, Michael and Specht, Günther},
  title     = {{Countering Plagiarism by Exposing Irregularities in Authors' Grammar}},
  booktitle = {Proceedings of the European Intelligence and Security Informatics Conference, (EISIC'2013)},
  year      = {2013},
  pages     = {15--22},
  publisher = {IEEE},
  doi       = {10.1109/EISIC.2013.10},
  isbn      = {9780769550626},
}

@InProceedings{Tschuggnall2013,
  author    = {Tschuggnall, Michael and Specht, G{\"{u}}nther},
  title     = {{Detecting Plagiarism in Text Documents through Grammar-Analysis of Authors}},
  booktitle = {15. GI-Fachtagung Datenbanksysteme f{\"{u}}r Business, Technologie und Web (BTW'2013)},
  year      = {2013},
  pages     = {241--259},
  isbn      = {978-3-88579-608-4},
  issn      = {16175468},
}

@Article{Stamatatos2013,
  author  = {Stamatatos, Efstathios},
  title   = {{On the Robustness of Authorship Attribution Based on Character N-Gram Features}},
  journal = {Journal of Law {\&} Policy},
  year    = {2013},
  pages   = {421--439},
  issn    = {10740635},
}

@InProceedings{Stamatatos2016,
  title     = {{Clustering by Authorship Within and Across Documents}},
  year      = {2016},
  publisher = {CEUR-WS.org},
  month     = {09},
  author    = {Stamatatos, Efstathios and Tschuggnall, Michael and Verhoeven, Ben and Daelemans, Walter and Specht, G{\"{u}}nther and Stein, Benno and Potthast, Martin},
  booktitle = {Working Notes of the Conference and Labs of the Evaluation forum (CLEF'2016)},
  pages     = {691--715},
}

@InProceedings{Stamatatos2017,
  author    = {Stamatatos, Efstathios},
  title     = {{Authorship Attribution Using Text Distortion}},
  booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL'2017)},
  year      = {2017},
  pages     = {1138--1149},
  month     = apr,
  publisher = {Association for Computational Linguistics},
  isbn      = {9781510838604},
}

@InProceedings{Tschuggnall2013b,
  author    = {Tschuggnall, Michael and Specht, Günther},
  title     = {{Using Grammar-profiles to Intrinsically Expose Plagiarism in Text Documents}},
  booktitle = {Proceedings of the 18th International Conference on Applications of Natural Language to Information Systems (NLDB'2013)},
  year      = {2013},
  editor    = {M{\'e}tais, Elisabeth and Meziane, Farid and Saraee, Mohamad and Sugumaran, Vijayan and Vadera, Sunil},
  pages     = {297--302},
  month     = {06},
  publisher = {Springer Berlin Heidelberg},
  isbn      = {9783642388231},
}

@InBook{Hollingsworth2012,
  chapter   = {2},
  pages     = {314--319},
  title     = {{Using Dependency-Based Annotations for Authorship Identification}},
  publisher = {Springer Berlin Heidelberg},
  year      = {2012},
  author    = {Hollingsworth, Charles},
  editor    = {Sojka, Petr and Hor{\'a}k, Ale{\v{s}} and Kope{\v{c}}ek, Ivan and Pala, Karel},
  isbn      = {978-3-642-32790-2},
  booktitle = {Text, Speech and Dialogue},
  doi       = {10.1007/978-3-642-32790-2\_38},
}

@Article{Augsten2010,
  author  = {Augsten, Nikolaus and Böhlen, Michael and Gamper, Johann},
  title   = {{PQ-Gram Distance Between Ordered Labeled Trees}},
  journal = {ACM Transactions on Database Systems},
  year    = {2010},
}

@Article{Tschuggnall2015,
  author  = {Tschuggnall, Michael and Specht, Günther},
  title   = {{On the Potential of Grammar Features for Automated Author Profiling}},
  journal = {International Journal on Advances in Intelligent Systems},
  year    = {2015},
  volume  = {8},
  number  = {3},
  pages   = {255--265},
}

@InBook{Sidorov2013,
  pages     = {1--11},
  title     = {{Syntactic Dependency-Based N-grams as Classification Features}},
  publisher = {Springer Heidelberg Berlin},
  year      = {2013},
  author    = {Sidorov, Grigori and Velasquez, Francisco and Stamatatos, Efstathios and Gelbukh, Alexander and Chanona-Hern{\'a}ndez, Liliana},
  editor    = {Batyrshin, Ildar and Mendoza, Miguel Gonz{\'a}lez},
  volume    = {11},
  series    = {Mexican International Conference on Artificial Intelligence (MICAI'2012)},
  booktitle = {Advances in Computational Intelligence},
  doi       = {10.1007/978-3-642-37798-3\_1},
}

@InProceedings{Tanguy2011,
  author    = {Tanguy, Ludovic and Urieli, Assaf and Calderone, Basilio},
  title     = {{A Multitude of Linguistically-Rich Features for Authorship Attribution}},
  booktitle = {Working Notes of the Conference and Labs of the Evaluation forum (CLEF'2017) Evaluation Labs},
  year      = {2011},
  volume    = {1177},
}

@InProceedings{Kaster2005,
  author    = {Kaster, Andreas and Siersdorfer, Stefan and Weikum, Gerhard},
  title     = {{Combining Text and Linguistic Document Representations for Authorship Attribution}},
  booktitle = {Working Notes of the 28th Conference on Research and Development in Information Retrieval (SIGIR'2005): Stylistic Analysis of Text for Information Access},
  year      = {2005},
  pages     = {27--35},
}

@InProceedings{Bykh2016,
  author    = {Bykh, Serhiy and Meurers, Detmar},
  title     = {{Advancing Linguistic Features and Insights by Label-informed Feature Grouping: An Exploration in the Context of Native Language Identification}},
  booktitle = {Proceedings of the 26th International Conference on Computational Linguistics (COLING'2016)},
  year      = {2016},
  pages     = {739--749},
  month     = dec,
  file      = {:Advancing Linguistic Features and Insights by Label-informed Feature Grouping - Bykh 2016.pdf:PDF},
}

@InProceedings{Ionescu2017,
  author    = {Ionescu, Radu Tudor and Popescu, Marius},
  title     = {{Can string kernels pass the test of time in Native Language Identification?}},
  booktitle = {Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications},
  year      = {2017},
  pages     = {224--234},
  month     = sep,
  publisher = {Association for Computational Linguistics},
  file      = {:Can string kernels pass the test of time in Native Language Identification - Ionescu 2017.pdf:PDF},
}

@InProceedings{Ionescu2014,
  author    = {Ionescu, Radu Tudor and Popescu, Marius and Cahill, Aoife},
  title     = {{Can characters reveal your native language? A language-independent approach to native language identification}},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP'2014)},
  year      = {2014},
  pages     = {1363--1373},
  month     = oct,
  publisher = {Association for Computational Linguistics},
  file      = {:Can characters reveal your native language - Ionescu 2014.pdf:PDF},
}

@InProceedings{Malmasi2014,
  author    = {Malmasi, Shervin and Dras, Mark},
  title     = {{Language Transfer Hypotheses with Linear SVM Weights}},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natuarl Language Processing (EMNLP'2014)},
  year      = {2014},
  pages     = {1385--1390},
  month     = oct,
  publisher = {Association for Computational Linguistics},
  file      = {:Language Transfer Hypotheses with Linear SVM Weights - Malmasi 2014.pdf:PDF},
}

@Article{Koppel2005,
  author  = {Koppel, Moshe and Schler, Jonathan and Zigdon, Kfir},
  title   = {{Automatically Determining an Anonymous Author's Native Language}},
  journal = {Intelligence and Security Informatics},
  year    = {2005},
  pages   = {41--76},
  issn    = {03029743},
  file    = {:home/benjamin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koppel, Schler, Zigdon - 2005 - Automatically determining an anonymous author's native language.pdf:pdf},
}

@Article{Stamatatos2009,
  author    = {Stamatatos, E.},
  title     = {{A Survey of Modern Authorship Attribution Methods}},
  journal   = {Journal of the American Society for Information Science and Technology},
  year      = {2009},
  volume    = {60},
  number    = {3},
  pages     = {538--556},
  month     = {March},
  doi       = {10.1002/asi.v60:3},
  numpages  = {19},
  publisher = {John Wiley \& Sons, Inc.},
}

@Article{Nirkhi2013,
  author        = {Nirkhi, Smita and Dharaskar, Rajiv V.},
  title         = {{Comparative Study of Authorship Identification Techniques for Cyber Forensics Analysis}},
  journal       = {International Journal of Advanced Computer Science \& Applications},
  year          = {2013},
  volume        = {4},
  number        = {5},
  date-added    = {2014-04-28 17:43:39 +0000},
  date-modified = {2014-04-28 17:43:39 +0000},
}

@InProceedings{Caliskan2012,
  author    = {Caliskan, Aylin and Greenstadt, Rachel},
  booktitle = {Proceedings of the 6th International Conference on Semantic Computing (ICSC'2012)},
  title     = {{Translate Once, Translate Twice, Translate Thrice and Attribute: Identifying Authors and Machine Translation Tools in Translated Text}},
  year      = {2012},
  month     = sep,
  pages     = {121--125},
  publisher = {IEEE},
  abstract  = {In this paper, we investigate the effects of machine translation tools on translated texts and the accuracy of authorship and translator attribution of translated texts. We show that the more translation performed on a text by a specific machine translation tool, the more effects unique to that translator are observed. We also propose a novel method to perform machine translator and authorship attribution of translated texts using a feature set that led to 91.13{\%} and 91.54{\%} accuracy on average, respectively. We claim that the features leading to highest accuracy in translator attribution are translator-dependent features and that even though translator-effect-heavy features are present in translated text, we can still succeed in authorship attribution. These findings demonstrate that stylometric features of the original text are preserved at some level despite multiple consequent translations and the introduction of translator-dependent features. The main contribution of our work is the discovery of a feature set used to accurately perform both translator and authorship attribution on a corpus of diverse topics from the twenty-first century, which has been consequently translated multiple times using machine translation tools.},
  doi       = {10.1109/ICSC.2012.46},
  file      = {:home/benjamin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Caliskan, Greenstadt - 2012 - Translate once, translate twice, translate thrice and attribute Identifying authors and machine translatio.pdf:pdf},
  isbn      = {978-1-4673-4433-3},
  journal   = {Proceedings of the 6th International Conference on Semantic Computing (ICSC'2012)},
  keywords  = {anonymity, authorship attribution, machine learning, machine translation, privacy},
}

@InProceedings{Rao2000,
  author    = {Rao, Josyula R. and Rohatgi, Pankaj},
  title     = {{Can Pseudonymity Really Guarantee Privacy?}},
  booktitle = {Proceedings of the 9th Conference on USENIX Security Symposium},
  year      = {2000},
  volume    = {9},
  series    = {SSYM'00},
  pages     = {7--7},
  month     = aug,
  publisher = {USENIX Association},
  acmid     = {1251313},
  location  = {Denver, Colorado},
  numpages  = {1},
}

@InProceedings{Aharoni2014,
  author    = {Aharoni, Roee and Koppel, Moshe and Goldberg, Yoav},
  title     = {{Automatic Detection of Machine Translated Text and Translation Quality Estimation}},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
  year      = {2014},
  volume    = {2},
  pages     = {289--295},
  month     = jun,
  publisher = {Association for Computational Linguistics},
  abstract  = {We show that it is possible to automatically detect machine translated text at sentence level from monolingual corpora, using text classification methods. We show further that the accuracy with which a learned classifier can detect text as machine translated is strongly correlated with the translation quality of the machine translation system that generated it. Finally, we offer a generic machine translation quality estimation technique based on this approach, which does not require reference sentences.},
  file      = {:home/benjamin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aharoni, Koppel, Goldberg - 2014 - Automatic Detection of Machine Translated Text and Translation Quality Estimation.pdf:pdf},
  isbn      = {9781937284732},
  journal   = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
}

@InProceedings{Goldhahn,
  author    = {Goldhahn, Dirk and Eckart, Thomas and Quasthoff, Uwe},
  booktitle = {Proceedings of the 8th International Conference on Language Ressources and Evaluation (LREC'2012)},
  title     = {{Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages}},
  year      = {2012},
  keywords  = {corpus creation, minority languages, text acquisition},
}

@InProceedings{Joachims1998,
  author    = {Joachims, Thorsten},
  title     = {{Text Categorization with Suport Vector Machines: Learning with Many Relevant Features}},
  booktitle = {Proceedings of the 10th European Conference on Machine Learning (ECML'1998)},
  year      = {1998},
  pages     = {137--142},
  month     = apr,
  publisher = {Springer Berlin Heidelberg},
  abstract  = {This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifes why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore, they are fully automatic, eliminating the need for manual parameter tuning.},
  doi       = {10.1007/BFb0026683},
  file      = {:home/benjamin/phd/literature/literature{\_}text/report23{\_}ps.pdf:pdf},
  isbn      = {3-540-64417-2},
  issn      = {0945-1129},
  journal   = {P10th European Conference on Machine Learning},
  pmid      = {9934216},
}

@InProceedings{Potthast2016,
  author    = {Martin Potthast and Matthias Hagen and Benno Stein},
  title     = {{Author Obfuscation: Attacking the State of the Art in Authorship Verification}},
  booktitle = {Working Notes Papers of the CLEF 2016 Evaluation Labs},
  year      = {2016},
  series    = {CEUR Workshop Proceedings},
  month     = sep,
  publisher = {CLEF and CEUR-WS.org},
  issn      = {1613-0073},
}

@Article{webenglish,
  author = {Pimienta, Daniel and Prado, Daniel and Blanco, Alvaro},
  title  = {{Twelve Years of Measuring Linguistic Diversity in the Internet: Balance and Perspectives}},
  year   = {2009},
  series = {UNESCO publications for the World Summit on the Information Society},
}

@InProceedings{Malmasi2017,
  author    = {Malmasi, Shervin and Evanini, Keelan and Cahill, Aoife and Tetreault, Joel and Pugh, Robert and Hamill, Christopher and Napolitano, Diane and Quan, Yao},
  title     = {A Report on the 2017 Native Language Identification Shared Task},
  booktitle = {Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications},
  year      = {2017},
  pages     = {62--75},
  month     = sep,
  publisher = {Association for Computational Linguistics},
}

@InProceedings{Swanson2013,
  author    = {Swanson, Ben and Charniak, Eugene},
  title     = {{Extracting the native language signal for second language acquisition}},
  booktitle = {Proceedings of NAACL-HLT},
  year      = {2013},
  pages     = {85--94},
  month     = jun,
  publisher = {Association for Computational Linguistics},
}

@InProceedings{Swanson2014,
  author    = {Swanson, Ben and Charniak, Eugene},
  title     = {{Data driven language transfer hypotheses}},
  booktitle = {Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics},
  year      = {2014},
  volume    = {2: short papers},
  pages     = {169--173},
  month     = apr,
  publisher = {Association for Computational Linguistics},
}

@InProceedings{Bykh2014,
  author    = {Bykh, Serhiy and Meurers, Detmar},
  title     = {Exploring syntactic features for native language identification: A variationist perspective on feature encoding and ensemble optimization},
  booktitle = {Proceedings of the 25th International Conference on Computational Linguistics (COLING'2014)},
  year      = {2014},
  pages     = {1962--1973},
  month     = aug,
}

@InProceedings{Bykh2012,
  author    = {Bykh, Serhiy and Meurers, Detmar},
  title     = {{Native Language Identification Using Recurring N-grams – Investigating Abstraction and Domain Dependence}},
  booktitle = {Proceedings of the 24th International Conference on Computational Linguistics (COLING'2012)},
  year      = {2012},
  pages     = {425--440},
  month     = dec,
}

@InProceedings{Koutini2017Genre,
  author    = {Koutini, Khaled and Imenina, Alina and Dorfer, Matthias and Gruber, Alexander Rudolf and Schedl, Markus},
  title     = {{MediaEval 2017 AcousticBrainz Genre Task: Multilayer Perceptron Approach}},
  booktitle = {CEURS Working Notes Proceedings of the MediaEval 2017 Workshop},
  year      = {2017},
  publisher = {CEUR-WS.org},
  location      = {Dublin, Ireland},
}

@InProceedings{Murauer2017Genre,
  author    = {Murauer, Benjamin and Mayerl, Maximilian and Tschuggnall, Michael and Zangerle, Eva and Pichl, Martin and Specht, G{\"u}nther},
  title     = {{Hierarchical Multilabel Classification and Voting for Genre Classification}},
  booktitle = {CEURS Working Notes Proceedings of the MediaEval 2017 Workshop},
  year      = {2017},
  publisher = {CEUR-WS.org},
  location      = {Dublin, Ireland},
}

@Article{Liu2017,
  author        = {Rui Liu and Junjie Hu and Wei Wei and Zi Yang and Eric Nyberg},
  title         = {Structural Embedding of Syntactic Trees for Machine Comprehension},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1703.00572},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, http://dblp.org},
  eprint        = {1703.00572},
  timestamp     = {Wed, 07 Jun 2017 14:42:22 +0200},
}

@InProceedings{Sapkota2014,
  author    = {Upendra Sapkota and Thamar Solorio and Manuel Montes-y-Gómez and Steven Bethard and Paolo Rosso},
  title     = {{Cross-Topic Authorship Attribution: Will Out-Of-Topic Data Help?}},
  booktitle = {Proceedings of the 25th International Conference on Computational Linguistics (COLING'2014)},
  year      = {2014},
  pages     = {1228--1237},
  month     = aug,
  location  = {Dublin, Ireland},
}

@Article{Luyckx2011,
  author   = {Luyckx, Kim and Daelemans, Walter},
  title    = {{The effect of author set size and data size in authorship attribution}},
  journal  = {Literary and Linguistic Computing},
  year     = {2011},
  volume   = {26},
  number   = {1},
  pages    = {35--55},
  issn     = {02681145},
  abstract = {Applications of authorship attribution ‘in the wild' (Koppel et al., 2011), for instance in social networks, will likely involve large sets of candidate authors and only limited data per author. In this article, we present the results of a systematic study of two important parameters in supervised machine learning that significantly affect performance in computational authorship attribution: (1) the number of candidate authors (i.e. the number of classes to be learned), and (2) the amount of training data available per candidate author (i.e. the size of the training data). We also investigate the robustness of different types of lexical and linguistic features to the effects of author set size and data size. The approach we take is an operationalization of the standard text categorization model, using memory-based learning for discriminating between the candidate authors. We performed authorship attribution experiments on a set of three benchmark corpora in which the influence of topic could be controlled. The short text fragments of e-mail length present the approach with a true challenge. Results show that, as expected, authorship attribution accuracy deteriorates as the number of candidate authors increases and size of training data decreases, although the machine learning approach continues performing significantly above chance. Some feature types (most notably character n-grams) are robust to changes in author set size and data size, but no robust individual features emerge.},
  doi      = {10.1093/llc/fqq013},
  file     = {:home/benjamin/Nextcloud/literature/Influence of Data Size on Authorship Attribution - Luyckx Daelemans.pdf:pdf},
  isbn     = {0268-1145},
}

@Article{Markov2017,
  author   = {Markov, Ilia and Stamatatos, Efstathios and Sidorov, Grigori},
  journal  = {Proceedings of the 18th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing'2017).},
  title    = {{Improving Cross-Topic Authorship Attribution: The Role of Pre-Processing}},
  year     = {2017},
  abstract = {The effectiveness of character n-gram features for represent-ing the stylistic properties of a text has been demonstrated in various independent Authorship Attribution (AA) studies. Moreover, it has been shown that some categories of character n-grams perform better than others both under single and cross-topic AA conditions. In this work, we present an improved algorithm for cross-topic AA. We demonstrate that the effectiveness of character n-grams representation can be significantly enhanced by performing simple pre-processing steps and appropriately tuning the number of features, especially in cross-topic conditions.},
  file     = {:home/benjamin/Nextcloud/literature/Improving Cross-Topic Authorship Attribution - The Role of Pre-Processing - Ilia Markov.pdf:pdf},
  keywords = {authorship attribution, charac-ter n-grams, cross-topic, machine learning, pre-processing},
}

@InProceedings{Sapkota2015,
  author    = {Sapkota, Upendra and Bethard, Steven and Montes, Manuel and Solorio, Thamar},
  title     = {{Not All Character N-grams Are Created Equal: A Study In Authorship Attribution}},
  booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human language Technologies},
  year      = {2015},
  pages     = {93--102},
  month     = jun,
  location  = {Denver, Colorado, USA},
}

@InProceedings{pan2011dataset,
  author    = {Argamon, Shlomo and Juola, Patrick},
  title     = {{Overview of the international authorship identification competition at PAN-2011.}},
  booktitle = {CLEF (Notebook Papers/Labs/Workshop)},
  year      = {2011},
}

@InProceedings{pan2012dataset,
  author    = {Juola, Patrick},
  title     = {{An Overview of the Traditional Authorship Attribution Subtask.}},
  booktitle = {CLEF (Online Working Notes/Labs/Workshop)},
  year      = {2012},
}

@InProceedings{stamatatos:2018,
  author    = {Efstathios Stamatatos and Francisco Rangel and Michael Tschuggnall and Mike Kestemont and Paolo Rosso and Benno Stein and Martin Potthast},
  title     = {{Overview of PAN-2018: Author Identification, Author Profiling, and Author Obfuscation}},
  booktitle = {Experimental IR Meets Multilinguality, Multimodality, and Interaction. 9th International Conference of the CLEF Initiative (CLEF 18)},
  year      = {2018},
  editor    = {Patrice Bellot and Chiraz Trabelsi and Josiane Mothe and Fionn Murtagh and {Jian Yun} Nie and Laure Soulier and Eric Sanjuan and Linda Cappellato and Nicola Ferro},
  month     = sep,
}

@InProceedings{kestemont:2018,
  author    = {Mike Kestemont and Michael Tschugnall and Efstathios Stamatatos and Walter Daelemans and G{\"u}nther Specht and Benno Stein and Martin Potthast},
  title     = {{Overview of the Author Identification Task at PAN-2018: Cross-domain Authorship Attribution and Style Change Detection}},
  booktitle = {Working Notes Papers of the CLEF 2018 Evaluation Labs},
  year      = {2018},
  editor    = {Linda Cappellato and Nicola Ferro and Jian-Yun Nie and Laure Soulier},
  month     = sep,
  issn      = {1613-0073},
  doi = {10.5281/zenodo.3737849},
}

@InProceedings{stein:2014j,
  author    = {Martin Potthast and Tim Gollub and Francisco Rangel and Paolo Rosso and Efstathios Stamatatos and Benno Stein},
  title     = {{Improving the Reproducibility of PAN's Shared Tasks: Plagiarism Detection, Author Identification, and Author Profiling}},
  booktitle = {Information Access Evaluation meets Multilinguality, Multimodality, and Visualization. 5th International Conference of the CLEF Initiative (CLEF 14)},
  year      = {2014},
  editor    = {Evangelos Kanoulas and Mihai Lupu and Paul Clough and Mark Sanderson and Mark Hall and Allan Hanbury and Elaine Toms},
  pages     = {268-299},
  month     = sep,
  publisher = {Springer},
  doi       = {10.1007/978-3-319-11382-1\_22},
  isbn      = {978-3-319-11381-4},
}

@InProceedings{menon2011domain,
  author    = {Menon, Rohith and Choi, Yejin},
  title     = {Domain independent authorship attribution without domain adaptation},
  booktitle = {Proceedings of the International Conference Recent Advances in Natural Language Processing 2011},
  year      = {2011},
  pages     = {309--315},
}

@Article{Posadas2017,
  author   = {Posadas-Dur{\'a}n, Juan-Pablo and G{\'o}mez-Adorno, Helena and Sidorov, Grigori and Batyrshin, Ildar and Pinto, David and Chanona-Hern{\'a}ndez, Liliana},
  title    = {Application of the distributed document representation in the authorship attribution task for small corpora},
  journal  = {Soft Computing},
  year     = {2017},
  volume   = {21},
  number   = {3},
  pages    = {627--639},
  month    = {Feb},
  issn     = {1433-7479},
  abstract = {Distributed word representation in a vector space (word embeddings) is a novel technique that allows to represent words in terms of the elements in the neighborhood. Distributed representations can be extended to larger language structures like phrases, sentences, paragraphs and documents. The capability to encode semantic information of texts and the ability to handle high- dimensional datasets are the reasons why this representation is widely used in various natural language processing tasks such as text summarization, sentiment analysis and syntactic parsing. In this paper, we propose to use the distributed representation at the document level to solve the task of the authorship attribution. The proposed method learns distributed vector representations at the document level and then uses the SVM classifier to perform the automatic authorship attribution. We also propose to use the word n-grams (instead of the words) as the input data type for learning the distributed representation model. We conducted experiments over six datasets used in the state-of-the-art works, and for the majority of the datasets, we obtained comparable or better results. Our best results were obtained using the combination of words and n-grams of words as the input data types. Training data are relatively scarce, which did not affect the distributed representation.},
  day      = {01},
  doi      = {10.1007/s00500-016-2446-x},
}

@Book{venuti2008translator,
  title     = {The translator's invisibility: A history of translation},
  publisher = {Routledge},
  year      = {2008},
  author    = {Venuti, Lawrence},
}

@InProceedings{bogdanova2014cross,
  author       = {Bogdanova, Dasha and Lazaridou, Angeliki},
  title        = {Cross-Language Authorship Attribution.},
  booktitle    = {Ninth International Conference on Language Resources and Evaluation (LREC'2014)},
  year         = {2014},
  pages        = {2015--2020},
}

@Article{eder2013corpussizes,
  author    = {Eder, Maciej},
  title     = {{Does size matter? Authorship attribution, small samples, big problem}},
  journal   = {Digital Scholarship in the Humanities},
  year      = {2013},
  volume    = {30},
  number    = {2},
  pages     = {167--182},
  publisher = {Oxford University Press},
}

@Article{gomez2018,
  author   = {G{\'o}mez-Adorno, Helena and Posadas-Dur{\'a}n, Juan-Pablo and Sidorov, Grigori and Pinto, David},
  title    = {Document embeddings learned on various types of n-grams for cross-topic authorship attribution},
  journal  = {Computing},
  year     = {2018},
  volume   = {100},
  number   = {7},
  pages    = {741--756},
  issn     = {1436-5057},
  abstract = {Recently, document embeddings methods have been proposed aiming at capturing hidden properties of the texts. These methods allow to represent documents in terms of fixed-length, continuous and dense feature vectors. In this paper, we propose to learn document vectors based on n-grams and not only on words. We use the recently proposed Paragraph Vector method. These n-grams include character n-grams, word n-grams and n-grams of POS tags (in all cases with n varying from 1 to 5). We considered the task of Cross-Topic Authorship Attribution and made experiments on The Guardian corpus. Experimental results show that our method outperforms word-based embeddings and character n-gram based linear models, which are among the most effective approaches for identifying the writing style of an author.},
  day      = {01},
  doi      = {10.1007/s00607-018-0587-8},
}

@InProceedings{llorens2016deep,
  author    = {Llorens, Marisa and Delany, Sarah Jane},
  title     = {Deep Level Lexical Features for Cross-lingual Authorship Attribution},
  booktitle = {Proceedings of the first Workshop on Modeling, Learning and Mining for Cross/Multilinguality},
  year      = {2016},
  pages     = {16--25},
  publisher = {Dublin Institute of Technology},
}

@Article{overdorf2016,
  author  = {Rebekah Overdorf and Rachel Greenstadt},
  title   = {{Blogs, Twitter Feeds, and Reddit Comments: Cross-domain Authorship Attribution}},
  journal = {Proceedings on Privacy Enhancing Technologies},
  year    = {2016},
  volume  = {2016},
  number  = {3},
  pages   = {155 -- 171},
}

@InProceedings{Narayanan2012,
  author    = {Arvind Narayanan and Hristo Paskov and Neil Zhenqiang Gong and John Bethencourt and Emil Stefanov and Eui Chul Richard Shin and Dawn Song},
  title     = {On the Feasibility of Internet-Scale Author Identification},
  booktitle = {2012 {IEEE} Symposium on Security and Privacy},
  year      = {2012},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/sp.2012.46},
}

@InProceedings{Koppel2006,
  author    = {Moshe Koppel and Jonathan Schler and Shlomo Argamon and Eran Messeri},
  title     = {Authorship attribution with thousands of candidate authors},
  booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
  year      = {2006},
  publisher = {{ACM} Press},
  doi       = {10.1145/1148170.1148304},
}

@Article{murauer2018dynamic,
  author  = {Murauer, Benjamin and Tschuggnall, Michael and Specht, G{\"u}nther},
  journal = {Working Notes of CLEF},
  title   = {{Dynamic Parameter Search for Cross-Domain Authorship Attribution}},
  year    = {2018},
}

@InCollection{Pereira2010,
  author    = {Rafael Corezola Pereira and Viviane P. Moreira and Renata Galante},
  title     = {A New Approach for Cross-Language Plagiarism Analysis},
  booktitle = {Multilingual and Multimodal Information Access Evaluation},
  publisher = {Springer Berlin Heidelberg},
  year      = {2010},
  pages     = {15--26},
  doi       = {10.1007/978-3-642-15998-5\_4},
}

@InProceedings{Stuart2013,
  author    = {Lauren M. Stuart and Saltanat Tazhibayeva and Amy R. Wagoner and Julia M. Taylor},
  title     = {Style Features for Authors in Two Languages},
  booktitle = {2013 {IEEE}/{WIC}/{ACM} International Joint Conferences on Web Intelligence ({WI}) and Intelligent Agent Technologies ({IAT})},
  year      = {2013},
  month     = {nov},
  publisher = {{IEEE}},
  doi       = {10.1109/wi-iat.2013.65},
}

@Article{Eder2011,
  author  = {Eder,Maciej},
  title   = {Style-markers in authorship attribution : a cross-language study of the authorial fingerprint},
  journal = {Studies in Polish Linguistics},
  year    = {2011},
  volume  = {6},
  number  = {1},
  pages   = {99-114},
}

@InProceedings{Vulic2015,
  author    = {Ivan Vuli{\'{c}} and Marie-Francine Moens},
  title     = {Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings},
  booktitle = {Proceedings of the 38th International Conference on Research and Development in Information Retrieval},
  year      = {2015},
  publisher = {{ACM} Press},
  doi       = {10.1145/2766462.2767752},
}

@Article{Vilares2016,
  author    = {Jes{\'{u}}s Vilares and Manuel Vilares and Miguel A. Alonso and Michael P. Oakes},
  title     = {On the feasibility of character n-grams pseudo-translation for Cross-Language Information Retrieval tasks},
  journal   = {Computer Speech {\&} Language},
  year      = {2016},
  volume    = {36},
  pages     = {136--164},
  month     = {mar},
  doi       = {10.1016/j.csl.2015.09.004},
  publisher = {Elsevier {BV}},
}

@InProceedings{Bhattacharya2016,
  author    = {Bhattacharya, Paheli and Goyal, Pawan and Sarkar, Sudeshna},
  title     = {Query Translation for Cross-Language Information Retrieval using Multilingual Word Clusters},
  booktitle = {Proceedings of the 6th Workshop on South and Southeast {A}sian Natural Language Processing ({WSSANLP}2016)},
  year      = {2016},
  pages     = {152--162},
  month     = dec,
  publisher = {The COLING 2016 Organizing Committee},
}

@InProceedings{Nie1999cross,
  author    = {Nie, Jian-Yun and Simard, Michel and Isabelle, Pierre and Durand, Richard},
  title     = {Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the Web},
  booktitle = {Proceedings of the 22nd annual international Conference on Research and development in information retrieval},
  year      = {1999},
  pages     = {74--81},
}

@InProceedings{koehn2005europarl,
  author    = {Koehn, Philipp},
  title     = {Europarl: A parallel corpus for statistical machine translation},
  booktitle = {MT summit},
  year      = {2005},
  volume    = {5},
  pages     = {79--86},
}

@InProceedings{Murauer2019,
  author    = {Murauer, Benjamin and Specht, G\"{u}nther},
  booktitle = {Proceedings of the 20th International Conference of the Cross-Language Evaluation Forum for European Languages (CLEF'2019)},
  title     = {{Generating Cross-Domain Text Classification Corpora from Social Media Comments}},
  year      = {2019},
  pages     = {114--125},
  doi       = {10.1007/978-3-030-28577-7\_7},
}

@PhdThesis{llorensDissertation,
  author = {Llorens-Salvador, Marisa},
  title  = {Lexical rIchness Feature Extraction method (LIFE) for Multilingual and Cross-lingual Authorship Attribution},
  school = {Dublin Institute of Technology},
  year   = {2018},
  type   = {dissertation},
  doi    = {10.21427/D7RJ29},
}

@InProceedings{pan2019,
  author    = {Walter Daelemans and Mike Kestemont and Enrique Manjavacas and Martin Potthast and Francisco Rangel and Paolo Rosso and G{\"{u}}nther Specht and Efstathios Stamatatos and Benno Stein and Michael Tschuggnall and Matti Wiegmann and Eva Zangerle},
  title     = {{Overview of PAN 2019: Author Profiling, Celebrity Profiling, Cross-domain Authorship Attribution and Style Change Detection}},
  booktitle = {Proceedings of the Tenth International Conference of the CLEF Association (CLEF 2019)},
  year      = {2019},
  editor    = {Fabio Crestani and Martin Braschler and Jacques Savoy and Andreas Rauber and Henning M{\"{u}}ller and {David {E.}} Losada and Gundula Heinatz and Linda Cappellato and Nicola Ferro},
  month     = sep,
  publisher = {Springer},
  site      = {Lugano, Switzerland},
  url       = {http://ceur-ws.org/Vol-2380/},
}

@InProceedings{Luyckx2005,
  author    = {Luyckx, Kim and Daelemans, Walter},
  title     = {{Shallow Text Analysis and Machine Learning for Authorship Attribution}},
  booktitle = {Proceedings of the 15th meeting of Computational Linguistics in the Netherlands},
  year      = {2005},
  pages     = {149--160},
  publisher = {LOT},
}

@Article{Zhang2015,
  author      = {Xiang Zhang and Junbo Zhao and Yann LeCun},
  title       = {Character-level Convolutional Networks for Text Classification},
  abstract    = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
  date        = {2015-09-04},
  eprint      = {http://arxiv.org/abs/1509.01626v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  keywords    = {cs.LG, cs.CL},
}

@Misc{Sanh2019,
  author        = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  month         = oct,
  title         = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  year          = {2019},
  abstract      = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  archiveprefix = {arXiv},
  eprint        = {1910.01108},
  keywords      = {cs.CL},
  primaryclass  = {cs.CL},
}

@Article{Liu2019,
  author        = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  title         = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
  year          = {2019},
  month         = jul,
  abstract      = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  eprint        = {1907.11692},
  file          = {:http\://arxiv.org/pdf/1907.11692v1:PDF},
  url          = {http://arxiv.org/pdf/1907.11692v1},
  keywords      = {cs.CL},
  primaryclass  = {cs.CL},
  journal = {},
}

@Article{Lample2019,
  author        = {Guillaume Lample and Alexis Conneau},
  title         = {Cross-lingual Language Model Pretraining},
  year          = {2019},
  month         = jan,
  abstract      = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.},
  archiveprefix = {arXiv},
  eprint        = {1901.07291},
  file          = {:http\://arxiv.org/pdf/1901.07291v1:PDF},
  keywords      = {cs.CL},
  primaryclass  = {cs.CL},
}

@Misc{Wu2019,
  author        = {Shijie Wu and Mark Dredze},
  month         = apr,
  title         = {Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT},
  year          = {2019},
  abstract      = {Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.},
  archiveprefix = {arXiv},
  eprint        = {1904.09077},
  file          = {:http\://arxiv.org/pdf/1904.09077v2:PDF},
  keywords      = {cs.CL},
  primaryclass  = {cs.CL},
}

@InProceedings{Zhang2018,
  author    = {Richong Zhang and Zhiyuan Hu and Hongyu Guo and Yongyi Mao},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  title     = {Syntax Encoding with Application in Authorship Attribution},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/d18-1294},
}

@Article{jafariakinabad2019,
  author        = {Fereshteh Jafariakinabad and Sansiri Tarnpradab and Kien A. Hua},
  title         = {Syntactic Recurrent Neural Network for Authorship Attribution},
  year          = {2019},
  month         = feb,
  abstract      = {Writing style is a combination of consistent decisions at different levels of language production including lexical, syntactic, and structural associated to a specific author (or author groups). While lexical-based models have been widely explored in style-based text classification, relying on content makes the model less scalable when dealing with heterogeneous data comprised of various topics. On the other hand, syntactic models which are content-independent, are more robust against topic variance. In this paper, we introduce a syntactic recurrent neural network to encode the syntactic patterns of a document in a hierarchical structure. The model first learns the syntactic representation of sentences from the sequence of part-of-speech tags. For this purpose, we exploit both convolutional filters and long short-term memories to investigate the short-term and long-term dependencies of part-of-speech tags in the sentences. Subsequently, the syntactic representations of sentences are aggregated into document representation using recurrent neural networks. Our experimental results on PAN 2012 dataset for authorship attribution task shows that syntactic recurrent neural network outperforms the lexical model with the identical architecture by approximately 14% in terms of accuracy.},
  archiveprefix = {arXiv},
  eprint        = {1902.09723},
  file          = {:http\://arxiv.org/pdf/1902.09723v2:PDF},
  keywords      = {cs.CL, cs.LG},
  primaryclass  = {cs.CL},
}

@InProceedings{Shrestha2017,
  author    = {Prasha Shrestha and Sebastian Sierra and Fabio Gonzalez and Manuel Montes and Paolo Rosso and Thamar Solorio},
  booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  title     = {Convolutional Neural Networks for Authorship Attribution of Short Texts},
  year      = {2017},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/e17-2106},
}

@misc{Tai2015,
      title={Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks}, 
      author={Kai Sheng Tai and Richard Socher and Christopher D. Manning},
      year={2015},
      eprint={1503.00075},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Jafari2019, 
  title={Style-aware Neural Model with Application in Authorship Attribution}, 
  author={Jafariakinabad, Fereshteh and Hua, Kien A}, 
  booktitle={2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)}, 
  pages={325--328}, 
  year={2019}, 
  organization={IEEE} }


@Article{Lample2019,
  author        = {Guillaume Lample and Alexis Conneau},
  title         = {Cross-lingual Language Model Pretraining},
  year          = {2019},
  month         = jan,
  abstract      = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.},
  archiveprefix = {arXiv},
  eprint        = {1901.07291},
  file          = {:http\://arxiv.org/pdf/1901.07291v1:PDF},
  keywords      = {cs.CL},
  primaryclass  = {cs.CL},
}

@Misc{Brown2020,
  author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  month         = may,
  title         = {{Language Models are Few-Shot Learners}},
  year          = {2020},
  abstract      = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  eprint        = {2005.14165},
  file          = {:http\://arxiv.org/pdf/2005.14165v4:PDF},
  keywords      = {cs.CL},
  primaryclass  = {cs.CL},
}

@Misc{Keung2019,
  author        = {Phillip Keung and Yichao Lu and Vikas Bhardwaj},
  title         = {{Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER}},
  year          = {2019},
  abstract      = {Contextual word embeddings (e.g. GPT, BERT, ELMo, etc.) have demonstrated state-of-the-art performance on various NLP tasks. Recent work with the multilingual version of BERT has shown that the model performs very well in zero-shot and zero-resource cross-lingual settings, where only labeled English data is used to finetune the model. We improve upon multilingual BERT's zero-resource cross-lingual performance via adversarial learning. We report the magnitude of the improvement on the multilingual MLDoc text classification and CoNLL 2002/2003 named entity recognition tasks. Furthermore, we show that language-adversarial training encourages BERT to align the embeddings of English documents and their translations, which may be the cause of the observed performance gains.},
  archiveprefix = {arXiv},
  eprint        = {1909.00153},
  file          = {:http\://arxiv.org/pdf/1909.00153v3:PDF},
  journal       = {arXiv.org},
  keywords      = {cs.CL, cs.LG},
  primaryclass  = {cs.CL},
}

@misc{devlin2018pretraining,
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  biburl = {https://www.bibsonomy.org/bibtex/210c860e3f390c6fbfd78a3b91ab9b0af/albinzehe},
  description = {[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  interhash = {a74f4c3853d3f0340e75546639134e91},
  intrahash = {10c860e3f390c6fbfd78a3b91ab9b0af},
  keywords = {bert elmo embeddings kallimachos nlp proposal-knowledge wordembeddings},
  timestamp = {2020-07-28T14:17:24.000+0200},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding},
  url = {http://arxiv.org/abs/1810.04805},
  year = 2018
}

@Misc{nivre2017universal,
  author = {Nivre, Joakim and Agi{\'c}, {\v{Z}}eljko and Ahrenberg, Lars and Antonsen, Lene and Aranzabe, Maria Jesus and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and Augustinus, Liesbeth and others},
  title  = {Universal Dependencies 2.1},
  year   = {2017},
  url    = {https://universaldependencies.org/},
}

@InProceedings{Nivre2016,
  author    = {Nivre, Joakim and De Marneffe, Marie-Catherine and Ginter, Filip and Goldberg, Yoav and Hajic, Jan and Manning, Christopher D and McDonald, Ryan and Petrov, Slav and Pyysalo, Sampo and Silveira, Natalia and others},
  booktitle = {Proceedings of the 10th Int. Conference on Language Resources and Evaluation (LREC'16)},
  title     = {Universal dependencies v1: A multilingual treebank collection},
  year      = {2016},
  pages     = {1659--1666},
}

@inproceedings{qi2020stanza,
 author = {Qi, Peng and Zhang, Yuhao and Zhang, Yuhui and Bolton, Jason and Manning, Christopher D.},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
 title = {Stanza: A {Python} Natural Language Processing Toolkit for Many Human Languages},
 url = {https://nlp.stanford.edu/pubs/qi2020stanza.pdf},
 year = {2020}
}

@InProceedings{mogadala2016bilingual,
  author    = {Mogadala, Aditya and Rettinger, Achim},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  title     = {Bilingual word embeddings from parallel and non-parallel corpora for cross-language text classification},
  year      = {2016},
  pages     = {692--702},
}

@InProceedings{karamanolakis2020cross,
  author    = {Karamanolakis, Giannis and Hsu, Daniel and Gravano, Luis},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  title     = {Cross-Lingual Text Classification with Minimal Resources by Transferring a Sparse Teacher},
  year      = {2020},
  pages     = {3604--3622},
}

@Article{rasooli2018cross,
  author    = {Rasooli, Mohammad Sadegh and Farra, Noura and Radeva, Axinia and Yu, Tao and McKeown, Kathleen},
  journal   = {Machine Translation},
  title     = {Cross-lingual sentiment transfer with limited resources},
  year      = {2018},
  number    = {1},
  pages     = {143--165},
  volume    = {32},
  publisher = {Springer},
}

@Article{vulic2019we,
  author  = {Vuli{\'c}, Ivan and Glava{\v{s}}, Goran and Reichart, Roi and Korhonen, Anna},
  url = {https://arxiv.org/pdf/1909.01638},
  title   = {Do we really need fully unsupervised cross-lingual embeddings?},
  year    = {2019},
}

@InProceedings{tschuggnall-etal-2019-reduce,
  author    = {Tschuggnall, Michael and Murauer, Benjamin and Specht, G{\"u}nther},
  booktitle = {Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)},
  title     = {{Reduce {\&} Attribute: Two-Step Authorship Attribution for Large-Scale Problems}},
  year      = {2019},
  month     = nov,
  pages     = {951--960},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/K19-1089},
}

@article{Seroussi2014,
author = {Seroussi, Yanir and Zukerman, Ingrid and Bohnert, Fabian},
title = {Authorship Attribution with Topic Models},
journal = {Computational Linguistics},
volume = {40},
number = {2},
pages = {269-310},
year = {2014},
doi = {10.1162/COLI\_a\_00173},

}

@inproceedings{Schler2006,
  title={Effects of age and gender on blogging.},
  author={Schler, Jonathan and Koppel, Moshe and Argamon, Shlomo and Pennebaker, James W},
  booktitle={AAAI spring symposium: Computational approaches to analyzing weblogs},
  volume={6},
  pages={199--205},
  year={2006}
}


@inproceedings{goldstein-stewart-etal-2008-creating,
    title = "Creating and Using a Correlated Corpus to Glean Communicative Commonalities",
    author = "Goldstein-Stewart, Jade  and
      Goodwin, Kerri  and
      Sabin, Roberta  and
      Winder, Ransom",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    publisher = "European Language Resources Association (ELRA)",
}


@misc{Liu2011reuters,
    title={{Reuter 50-50 Dataset}},
    author={Zhi Liu},
    howpublished={National Engineering Research Center for E-Learning Technology China},
    year={2011},
    url={https://archive.ics.uci.edu/ml/datasets/Reuter_50_50},
}

@InProceedings{mariannmt,
  title     = {Marian: Fast Neural Machine Translation in {C++}},
  author    = {Junczys-Dowmunt, Marcin and Grundkiewicz, Roman and
               Dwojak, Tomasz and Hoang, Hieu and Heafield, Kenneth and
               Neckermann, Tom and Seide, Frank and Germann, Ulrich and
               Fikri Aji, Alham and Bogoychev, Nikolay and
               Martins, Andr\'{e} F. T. and Birch, Alexandra},
  booktitle = {Proceedings of ACL 2018, System Demonstrations},
  pages     = {116--121},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  month     = {July},
  url       = {http://www.aclweb.org/anthology/P18-4020}
}

@inproceedings{imdb62dataset,
author = {Seroussi, Yanir and Zukerman, Ingrid and Bohnert, Fabian},
title = {{Collaborative Inference of Sentiments from Texts}},
booktitle={Proceedings of the 18th International Conference on User Modeling, Adaptation and Personalization (UMOD'2010)},
year = {2010},
month = {06},
pages = {195-206},
doi = {10.1007/978-3-642-13470-8_19}
}

@inproceedings{zhang2020n,
  title={n-BiLSTM: BiLSTM with n-gram Features for Text Classification},
  author={Zhang, Yunxiang and Rao, Zhuyi},
  booktitle={2020 IEEE 5th Information Technology and Mechatronics Engineering Conference (ITOEC)},
  pages={1056--1059},
  year={2020},
  organization={IEEE}
}

@inproceedings{sari2018topicorstyle,
    title = "Topic or Style? Exploring the Most Useful Features for Authorship Attribution",
    author = "Sari, Yunita  and
      Stevenson, Mark  and
      Vlachos, Andreas",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1029",
    pages = "343--353",
}

@article{grieve2007QuantitativeAA,
  title={Quantitative Authorship Attribution: An Evaluation of Techniques},
  author={Grieve, Jack},
  journal={Literary and Linguistic Computing},
  year={2007},
  volume={22},
  pages={251-270}
}  

@Article{eder2013corpuserrors,
  author    = {Eder, Maciej},
  journal   = {Literary and Linguistic Computing},
  title     = {Mind your corpus: systematic errors in authorship attribution},
  year      = {2013},
  month     = {jul},
  number    = {4},
  pages     = {603--614},
  volume    = {28},
  doi       = {10.1093/llc/fqt039},
  publisher = {Oxford University Press},
}

@Article{eder2012birds,
  author    = {Eder, Maciej and Rybicki, Jan},
  journal   = {Literary and Linguistic Computing},
  title     = {Do birds of a feather really flock together, or how to choose training samples for authorship attribution},
  year      = {2012},
  number    = {2},
  pages     = {229--236},
  volume    = {28},
  publisher = {The European Association for Digital Humanities},
}

@InProceedings{Fabien2020,
  author    = {Fabien, Ma\"{e}l and Villatoro-Tello, Esaú and Motlicek, Petr and Parida, Shantipriya},
  booktitle = {Proceedings of the 17th International Conference on Natural Language Processing},
  title     = {{BertAA: BERT fine-tuning for Authorship Attribution}},
  year      = {2020},
  url       = {http://infoscience.epfl.ch/record/285045},
}

@InProceedings{Barlas2020,
  author    = {Georgios Barlas and Efstathios Stamatatos},
  booktitle = {{IFIP} Advances in Information and Communication Technology},
  title     = {{Cross-Domain Authorship Attribution Using Pre-trained Language Models}},
  year      = {2020},
  pages     = {255--266},
  doi       = {10.1007/978-3-030-49161-1_22},
}

@Article{lewis2004rcv1,
  author    = {Lewis, David D and Yang, Yiming and Russell-Rose, Tony and Li, Fan},
  journal   = {Journal of machine learning research},
  title     = {Rcv1: A new benchmark collection for text categorization research},
  year      = {2004},
  number    = {Apr},
  pages     = {361--397},
  volume    = {5},
  publisher = {Goldsmiths, University of London},
}

@InProceedings{murauer2021smallscale,
  author    = {Murauer, Benjamin and Specht, Günther},
  booktitle = {Proceedings of the 18th Machine Translation Summit: 4th Workshop on Technologies for MT of Low Resource Languages},
  title     = {{Small-Scale Cross-Language AuthorshipAttribution on Social Media Comments}},
  year      = {2021},
  pages     = {11--19},
}

@Article{marcus1993penntreebank,
  author     = {Marcus, Mitchell P. and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
  journal    = {Comput. Linguist.},
  title      = {{Building a Large Annotated Corpus of English: The Penn Treebank}},
  year       = {1993},
  issn       = {0891-2017},
  month      = jun,
  number     = {2},
  pages      = {313–330},
  volume     = {19},
  issue_date = {June 1993},
  numpages   = {18},
  publisher  = {MIT Press},
}

@Article{brants2004tigercorpus,
  author  = {Sabine Brants and Stefanie Dipper and Peter Eisenberg and Silvia Hansen and Esther König and Wolfgang Lezius and Christian Rohrer and George Smith and Hans Uszkoreit},
  journal = {Journal of Language and Computation},
  title   = {{TIGER: Linguistic Interpretation of a German Corpus.}},
  year    = {2004},
  pages   = {597--620},
  volume  = {2},
  url     = {https://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/tiger/},
}

@Book{kucera1967browncorpus,
  author    = {Kucera, Henry and Nelson, Francis W.},
  publisher = {Brown University Press},
  title     = {{Computational analysis of present-day American English}},
  year      = {1967},
  isbn      = {9780870571053},
}

@Article{Barlas2021,
  author    = {Georgios Barlas and Efstathios Stamatatos},
  journal   = {Evolving Systems},
  title     = {A transfer learning approach to cross-domain authorship attribution},
  year      = {2021},
  month     = {apr},
  doi       = {10.1007/s12530-021-09377-2},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{marneffe2014universal,
  author    = {de Marneffe, Marie-Catherine and Dozat, Timothy and Silveira, Natalia and Haverinen, Katri and Ginter, Filip and Nivre, Joakim and Manning, Christopher D.},
  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},
  title     = {Universal {S}tanford dependencies: A cross-linguistic typology},
  year      = {2014},
  pages     = {4585--4592},
  abstract  = {Revisiting the now de facto standard Stanford dependency representation, we propose an improved taxonomy to capture grammatical relations across languages, including morphologically rich ones. We suggest a two-layered taxonomy: a set of broadly attested universal grammatical relations, to which language-specific relations can be added. We emphasize the lexicalist stance of the Stanford Dependencies, which leads to a particular, partially new treatment of compounding, prepositions, and morphology. We show how existing dependency schemes for several languages map onto the universal taxonomy proposed here and close with consideration of practical implications of dependency representation choices for NLP applications, in particular parsing.},
  url       = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/1062_Paper.pdf},
}

@InProceedings{pan2015profiling,
  author    = {Francisco Manuel Rangel Pardo and Fabio Celli and Paolo Rosso and Martin Potthast and Benno Stein and Walter Daelemans},
  booktitle = {CEUR Workshop Proceedings},
  title     = {{Overview of the 3rd Author Profiling Task at PAN 2015}},
  year      = {2015},
  editor    = {Cappellato, L. and Ferro, N. and Gareth, J. and San Juan, E.},
  url       = {https://ceur-ws.org},
}

@InProceedings{murauer2021authbench,
  author    = {Murauer, Benjamin and Specht, G\"{u}nther},
  booktitle = {Proceedings of the Second Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP'21@)},
  title     = {{Developing a Benchmark for Reducing Data Bias in Authorship Attribution}},
  year      = {2021},
  comment   = {to be published},
}

@InProceedings{murauer2021dtgrams,
  author    = {Murauer, Benjamin and Specht, G\"{u}nther},
  booktitle = {Proceedings of the 32nd GI-Workshop Grundlagen von Datenbanksysteme (GvDB'21)},
  title     = {{DT-grams: Structured Dependency Grammar Stylometry for Cross-Language Authorship Attribution}},
  year      = {2021},
}

  
@Article{lodhi2002,
  author  = {Lodhi, Huma and Saunders, Craig and Shawe-Taylor, John and Cristianini, Nello and Watkins, Chris},
  journal = {Journal of Machine Learning Research},
  title   = {Text classification using string kernels},
  year    = {2002},
  number  = {Feb},
  pages   = {419--444},
  volume  = {2},
}

@Article{cancedda2003,
  author    = {Cancedda, Nicola and Gaussier, Eric and Goutte, Cyril and Renders, Jean Michel},
  journal   = {The Journal of Machine Learning Research},
  title     = {Word sequence kernels},
  year      = {2003},
  pages     = {1059--1082},
  volume    = {3},
  publisher = {JMLR. org},
}

@Article{ionescu2016,
  author  = {Ionescu, Radu Tudor and Popescu, Marius and Cahill, Aoife},
  journal = {Computational Linguistics},
  title   = {String kernels for native language identification: Insights from behind the curtains},
  year    = {2016},
  number  = {3},
  pages   = {491--525},
  volume  = {42},
}

@InProceedings{popescu2012kernel,
  author    = {Popescu, Marius and Grozea, Cristian},
  booktitle = {{CLEF} 2012 Evaluation Labs and Workshop, Online Working Notes},
  title     = {Kernel Methods and String Kernels for Authorship Analysis.},
  year      = {2012},
  publisher = {CEUR-WS.org},
  volume    = {1178},
  url       = {http://ceur-ws.org/Vol-1178/CLEF2012wn-PAN-PopescuEt2012.pdf},
}

@Article{Sebastiani2002,
  author    = {Fabrizio Sebastiani},
  journal   = {{ACM} Computing Surveys},
  title     = {{Machine Learning in Automated Text Categorization}},
  year      = {2002},
  month     = {mar},
  number    = {1},
  pages     = {1--47},
  volume    = {34},
  doi       = {10.1145/505282.505283},
  publisher = {Association for Computing Machinery ({ACM})},
}

@Article{Koppel2009,
  author    = {Moshe Koppel and Jonathan Schler and Shlomo Argamon},
  journal   = {Journal of the American Society for Information Science and Technology},
  title     = {{Computational Methods in Authorship Attribution}},
  year      = {2009},
  month     = {jan},
  number    = {1},
  pages     = {9--26},
  volume    = {60},
  doi       = {10.1002/asi.20961},
  publisher = {Wiley},
}

@Article{yang2014,
  author  = {Yang, Min and Chow, Kam-Pui},
  journal = {IFIP Advances in Information and Communication Technology},
  title   = {{Authorship Attribution for Forensic Investigation with Thousands of Authors}},
  year    = {2014},
  month   = {06},
  pages   = {339-350},
  volume  = {428},
  doi     = {10.1007/978-3-642-55415-5_28},
  isbn    = {978-3-642-55414-8},
}

@Article{argamon2018computational,
  author  = {Argamon, Shlomo},
  journal = {Language and Law / Linguagem e Direito},
  title   = {{Computational Forensic Authorship Analysis: Promises and Pitfalls}},
  year    = {2018},
  number  = {2},
  pages   = {7--37},
  volume  = {5},
}

@Article{fobbe2020,
  author  = {Fobbe, Eilika},
  journal = {International Journal of Language and Law},
  title   = {{Text-Linguistic Analysis in Forensic Authorship Attribution}},
  year    = {2020},
  pages   = {93--114},
  volume  = {9},
  doi     = {10.14762/jll.2020.093},
}

@Article{silva2018,
  author  = {Sousa-Silva, Rui},
  journal = {Language and Law / Linguagem e Direito},
  title   = {{Computational Forensic Linguistics: An Overview of Computational Applications in Forensic Contexts}},
  year    = {2018},
  month   = {12},
  number  = {2},
  pages   = {118-143},
  volume  = {5},
}

@PhdThesis{pobitzer,
  author = {Philipp Pobitzer},
  school = {Universität Innsbruck},
  title  = {{Design and Evaluation of Different pq-gram Variants for Grammar-Based Text Analysis}},
  year   = {2017},
  type   = {Bachelor Thesis},
}

@InProceedings{Chen_2016,
  author    = {Tianqi Chen and Carlos Guestrin},
  booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
  title     = {{XGBoost: A Scalable Tree Boosting System}},
  year      = {2016},
  month     = {aug},
  pages     = {785--794},
  publisher = {{ACM}},
  doi       = {10.1145/2939672.2939785},
}

@Article{sadeniemi2008,
  author  = {Sadeniemi, Markus and Kettunen, Kimmo and Lindh-Knuutila, Tiina and Honkela, Timo},
  journal = {Journal of Quantitative Linguistics},
  title   = {{Complexity of European Union Languages: A Comparative Approach}},
  year    = {2008},
  number  = {2},
  pages   = {185--211},
  volume  = {15},
  doi     = {10.1080/09296170801961843},
}

@Article{Koppel2011,
  author    = {Moshe Koppel and Jonathan Schler and Shlomo Argamon},
  journal   = {Language Resources and Evaluation},
  title     = {Authorship attribution in the wild},
  year      = {2011},
  month     = {jan},
  number    = {1},
  pages     = {83--94},
  volume    = {45},
  doi       = {10.1007/s10579-009-9111-2},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Porter1980,
  author  = {Martin Porter},
  journal = {Program},
  title   = {An algorithm for suffix stripping},
  year    = {2006},
  number  = {3},
  pages   = {130--137},
  volume  = {14},
  doi     = {10.1108/00330330610681286},
}

@Book{Manning1999,
  author    = {Manning, Christopher and Schütze, Hinrich},
  publisher = {MIT Press},
  title     = {{Foundations of Statistical Natural Language Processing}},
  year      = {1999},
  isbn      = {0262133601},
  month     = may,
  ean       = {9780262133609},
  pagetotal = {720},
}

@Misc{Mikolov2013,
  author        = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  month         = jan,
  title         = {{Efficient Estimation of Word Representations in Vector Space}},
  year          = {2013},
  abstract      = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  eprint        = {1301.3781},
  file          = {:http\://arxiv.org/pdf/1301.3781v3:PDF},
  keywords      = {cs.CL},
  primaryclass  = {cs.CL},
}

@InProceedings{Pennington2014,
  author    = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title     = {{GloVe: Global Vectors for Word Representation}},
  year      = {2014},
  pages     = {1532--1543},
  url       = {http://www.aclweb.org/anthology/D14-1162},
}

@Misc{Bojanowski2016,
  author        = {Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},
  month         = jul,
  title         = {{Enriching Word Vectors with Subword Information}},
  year          = {2016},
  abstract      = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  archiveprefix = {arXiv},
  eprint        = {1607.04606},
  file          = {:http\://arxiv.org/pdf/1607.04606v2:PDF},
  keywords      = {cs.CL, cs.LG},
  primaryclass  = {cs.CL},
}

@InProceedings{Le2014,
  author       = {Le, Quoc and Mikolov, Tomas},
  booktitle    = {Proceedings of the 31st International Conference on Machine Learning},
  title        = {{Distributed Representations of Sentences and Documents}},
  year         = {2014},
  organization = {PMLR},
  pages        = {1188--1196},
}

@Article{Pogorelov2021,
  author  = {Pogorelov, Konstantin and Schroeder, Daniel Thilo and Filkuková, Petra and Brenner, Stefan and Langguth, Johannes},
  journal = {Proceedings of the 2021 Workshop on Open Challenges in Online SocialNetworks},
  title   = {{WICO Text: A Labeled Datasetof Conspiracy Theory and 5G-Corona Misinformation Tweets}},
  year    = {2021},
  pages   = {21--25},
  doi     = {10.1145/3472720.3483617},
}

@Article{Pogorelov2022Task,
  author  = {Pogorelov, Konstantin and Schroeder, Daniel Thilo and Brenner, Stefan and Langguth, Johannes},
  journal = {CEURS Working Notes Proceedings of the MediaEval 2021 Workshop},
  title   = {{FakeNews: Corona Virus and Conspiracies Multimedia AnalysisTask at MediaEval 2021}},
  year    = {2022},
}

@InProceedings{mourad2013subjectivity,
  author    = {Mourad, Ahmed and Darwish, Kareem},
  booktitle = {Proceedings of the 4th workshop on computational approaches to subjectivity, sentiment and social media analysis},
  title     = {{Subjectivity and Sentiment Analysis of Modern Standard Arabic and ArabicMicroblogs}},
  year      = {2013},
  pages     = {55--64},
}

@InProceedings{spencer2012sentimentor,
  author    = {Spencer, James and Uchyigit, Gulden},
  booktitle = {Proceedings of the 1st International Workshop on Sentiment Discovery from Affective Data at European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases},
  title     = {{Sentimentor: Sentiment Analysis of Twitter Data}},
  year      = {2012},
  pages     = {56--66},
}

@InProceedings{Raghuvanshi2016,
  author    = {Neha Raghuvanshi and Jaikumar M. Patil},
  booktitle = {2016 International Conference on Electrical, Electronics, and Optimization Techniques ({ICEEOT})},
  title     = {{A Brief Review on Sentiment Analysis}},
  year      = {2016},
  month     = {mar},
  publisher = {{IEEE}},
  doi       = {10.1109/iceeot.2016.7755213},
}

@Misc{Budzianowski2019,
  author        = {Paweł Budzianowski and Ivan Vulić},
  month         = jul,
  title         = {{Hello, It's GPT-2 -- How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems}},
  year          = {2019},
  abstract      = {Data scarcity is a long-standing and crucial challenge that hinders quick development of task-oriented dialogue systems across multiple domains: task-oriented dialogue models are expected to learn grammar, syntax, dialogue reasoning, decision making, and language generation from absurdly small amounts of task-specific data. In this paper, we demonstrate that recent progress in language modeling pre-training and transfer learning shows promise to overcome this problem. We propose a task-oriented dialogue model that operates solely on text input: it effectively bypasses explicit policy and language generation modules. Building on top of the TransferTransfo framework (Wolf et al., 2019) and generative model pre-training (Radford et al., 2019), we validate the approach on complex multi-domain task-oriented dialogues from the MultiWOZ dataset. Our automatic and human evaluations show that the proposed model is on par with a strong task-specific neural baseline. In the long run, our approach holds promise to mitigate the data scarcity problem, and to support the construction of more engaging and more eloquent task-oriented conversational agents.},
  archiveprefix = {arXiv},
  eprint        = {1907.05774},
  file          = {:http\://arxiv.org/pdf/1907.05774v2:PDF},
  keywords      = {cs.CL},
  primaryclass  = {cs.CL},
}

@Misc{Wu2020,
  author        = {Chien-Sheng Wu and Steven Hoi and Richard Socher and Caiming Xiong},
  month         = apr,
  title         = {{TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue}},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2004.06871},
  file          = {:http\://arxiv.org/pdf/2004.06871v3:PDF},
  keywords      = {cs.CL},
  primaryclass  = {cs.CL},
}

@InCollection{Sun2019,
  author    = {Chi Sun and Xipeng Qiu and Yige Xu and Xuanjing Huang},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  title     = {{How to Fine-Tune BERT for Text Classification?}},
  year      = {2019},
  pages     = {194--206},
  doi       = {10.1007/978-3-030-32381-3_16},
}

@Article{Murauer2021,
  author        = {Benjamin Murauer and Michael Tschuggnall and Günther Specht},
  title         = {{On the Influence of Machine Translation on Language Origin Obfuscation}},
  year          = {2018},
  month         = jun,
  abstract      = {In the last decade, machine translation has become a popular means to deal with multilingual digital content. By providing higher quality translations, obfuscating the source language of a text becomes more attractive. In this paper, we analyze the ability to detect the source language from the translated output of two widely used commercial machine translation systems by utilizing machine-learning algorithms with basic textual features like n-grams. Evaluations show that the source language can be reconstructed with high accuracy for documents that contain a sufficient amount of translated text. In addition, we analyze how the document size influences the performance of the prediction, as well as how limiting the set of possible source languages improves the classification accuracy.},
  archiveprefix = {arXiv},
  eprint        = {2106.12830},
  file          = {:http\://arxiv.org/pdf/2106.12830v1:PDF},
  keywords      = {cs.CL},
  primaryclass  = {cs.CL},
}

@Article{Garcia2006,
  author    = {A. M. Garcia and J. C. Martin},
  journal   = {Literary and Linguistic Computing},
  title     = {{Function Words in Authorship Attribution Studies}},
  year      = {2006},
  month     = {nov},
  number    = {1},
  pages     = {49--66},
  volume    = {22},
  doi       = {10.1093/llc/fql048},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Holmes1994,
  author  = {Holmes, David I.},
  journal = {Computers and the Humanities},
  title   = {{Authorship Attribution}},
  year    = {1994},
  number  = {2},
  pages   = {87--106},
  volume  = {28},
}

@InProceedings{Argamon2005,
  author    = {Argamon, Shlomo and Levitan, Shlomo},
  booktitle = {Proceedings of the Joint Conference of the Association for Computers and the Humanities and the Association for Literary and Linguistic Computing},
  title     = {Measuring the usefulness of function words for authorship attribution},
  year      = {2005},
  pages     = {1--3},
}

@InProceedings{Raafat2021,
  author    = {Maryam A. Raafat and Rania Abdel-Fattah El-Wakil and Ayman Atia},
  booktitle = {2021 International Mobile, Intelligent, and Ubiquitous Computing Conference ({MIUCC})},
  title     = {{Comparative study for Stylometric analysis techniques for authorship attribution}},
  year      = {2021},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/miucc52538.2021.9447600},
}

@InCollection{Liu2021,
  author    = {Jianbo Liu and Zhiqiang Hu and Jiasheng Zhang and Roy Ka-Wei Lee and Jie Shao},
  booktitle = {Web Information Systems Engineering {\textendash} {WISE} 2021},
  publisher = {Springer International Publishing},
  title     = {{A Syntax-Aware Encoder for Authorship Attribution}},
  year      = {2021},
  pages     = {403--411},
  doi       = {10.1007/978-3-030-90888-1_31},
}

@InProceedings{Lagutina2019,
  author    = {Lagutina, Ksenia and Lagutina, Nadezhda and Boychuk, Elena and Vorontsova, Inna and Shliakhtina, Elena and Belyaeva, Olga and Paramonov, Ilya and Demidov, P.G.},
  booktitle = {Proceedings of the 25th Conference of Open Innovations Association (FRUCT)},
  title     = {{A Survey on Stylometric Text Features}},
  year      = {2019},
  pages     = {184-195},
  doi       = {10.23919/FRUCT48121.2019.8981504},
}

@Comment{jabref-meta: databaseType:bibtex;}
